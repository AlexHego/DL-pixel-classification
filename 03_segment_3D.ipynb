{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e0bb5641-5134-454e-a77c-9f23e416b475",
   "metadata": {},
   "source": [
    "# Script to segment your 3D data \n",
    "3D Image Segmentation Script using DL-pixel classification based on U-net\n",
    "\n",
    "This script performs slice-by-slice inference on volumetric microscopy images \n",
    "using a pretrained PyTorch model. It supports multiple input formats including \n",
    "CZI, TIFF, and OME-TIFF, and produces multi-channel OME-TIFF outputs containing \n",
    "either classification masks or class probability maps.\n",
    "\n",
    "Features:\n",
    "- Sliding window inference for large 2D slices (Z-stack compatible)\n",
    "- Export options: classification masks (binary per class) or probability maps\n",
    "- Selectable output classes (EXPORT_CLASSES)\n",
    "- Memory-efficient progressive writing to avoid RAM overload\n",
    "- Output format compatible with Fiji, Napari, Imaris, etc.\n",
    "\n",
    "Note:\n",
    "Class indices start at 0 (PyTorch convention), while annotation tools like Napari \n",
    "typically use labels starting at 1. Adjust EXPORT_CLASSES accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d680aac8-9616-4ca3-800b-5b03402ff055",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "raster_geometry not imported.  This is only needed for the ellipsoid rendering in apply_stardist\n"
     ]
    }
   ],
   "source": [
    "## import librairies\n",
    "import torch\n",
    "import sys\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import tifffile\n",
    "from tifffile import TiffWriter, imwrite, TiffFile\n",
    "import czifile\n",
    "import gc\n",
    "from tqdm import tqdm\n",
    "from monai.inferers import sliding_window_inference\n",
    "from tnia.deeplearning.dl_helper import quantile_normalization\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d0382d9-116f-40cd-a60b-9eedf181009e",
   "metadata": {},
   "source": [
    "### Configuration section\n",
    "Define base directories, model path, input/output folders. Make sure these paths and options match your system and use case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2de9d645-ba59-4116-8ebe-d58c4ff371b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_PATH = Path(r'C:/Users/Alex/Desktop/Mailis')\n",
    "DATA_DIR = BASE_PATH / \"data\"\n",
    "MODEL_PATH = BASE_PATH / \"models\" / \"vessel_final_3.pth\"\n",
    "OUTPUT_DIR = BASE_PATH / \"vessel_binary\"\n",
    "\n",
    "ROI_SIZE = 512\n",
    "BATCH_SIZE = 1\n",
    "SUPPORTED_EXTENSIONS = [\n",
    "    \"*.czi\", \"*.tif\", \"*.tiff\", \"*.ome.tif\", \"*.ome.tiff\", \"*.btf\", \"*.ome.btf\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08d757ee-a1fb-4121-8d1d-f82c8ff93aab",
   "metadata": {},
   "source": [
    "### Model initialization and Count number of trained classes\n",
    "Load the trained model and move it to the appropriate device (CPU or GPU). Then, detect how many output classes the model was trained with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f0b4147c-1ab9-40d5-b2df-4869b9dfe86b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model was trained with 3 classes (class indices: 0 to 2).\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = torch.load(MODEL_PATH, weights_only=False).to(device)\n",
    "model.eval() # Switch the model to evaluation mode so layers like BatchNorm and Dropout behave correctly during inference.               \n",
    "torch.set_grad_enabled(False)\n",
    "# --- Detect number of output classes from the model ---\n",
    "with torch.no_grad():\n",
    "    dummy_input = torch.zeros(1, 1, ROI_SIZE, ROI_SIZE, device=device)\n",
    "    dummy_output = sliding_window_inference(dummy_input, roi_size=ROI_SIZE, sw_batch_size=1, predictor=model)\n",
    "    num_model_classes = dummy_output.shape[1]\n",
    "\n",
    "print(f\"Model was trained with {num_model_classes} classes (class indices: 0 to {num_model_classes - 1}).\")\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71e18d5e-3083-45fb-8609-029d33f6a0ca",
   "metadata": {},
   "source": [
    "### Choose output mode and classes to export\n",
    "\n",
    "Choose between \"classification\" (argmax per pixel) and \"probability\" (raw softmax scores).\n",
    "Then, select which class indices you want to export in the final output.\n",
    "\n",
    "NOTE:\n",
    "In Napari, annotation labels usually start at 1. However, model outputs are indexed from 0. So Napari class 1 corresponds to model class index 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "45a653ae-f9d1-4c20-a47c-f26b82972ff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "EXPORT_MODE = \"probability\"  # \"classification\" or \"probability\"\n",
    "EXPORT_CLASSES = [0,1,2]       # These are model class indices (ex [0, 1],..., [1, 2, 3] )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b20d794-d98e-4444-83aa-32f4524de84b",
   "metadata": {},
   "source": [
    "### Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f1f6eaf-f7f1-463d-b474-2b988613be3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13 image file(s) detected.\n",
      "\n",
      "\n",
      "[1/13] Processing: M20E4-Stitching-09-Create Image Subset-04.czi\n"
     ]
    }
   ],
   "source": [
    "# IMAGE LOADING\n",
    "\n",
    "def load_image(path: Path) -> np.ndarray:\n",
    "    ext = path.suffix.lower()\n",
    "    if ext == \".czi\":\n",
    "        image = czifile.imread(path)\n",
    "    elif ext in [\".tif\", \".tiff\", \".ome.tif\", \".ome.tiff\", \".btf\", \".ome.btf\"]:\n",
    "        image = tifffile.imread(path)\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported file format: {ext}\")\n",
    "    \n",
    "    image = np.squeeze(image)\n",
    "    if image.ndim != 3:\n",
    "        raise ValueError(f\"Expected 3D image (Z, Y, X), got shape {image.shape}\")\n",
    "    return image\n",
    "\n",
    "\n",
    "# PREDICTION\n",
    "\n",
    "def predict(image_2d: np.ndarray, model: torch.nn.Module) -> np.ndarray:\n",
    "    image = quantile_normalization(image_2d).astype(np.float32)\n",
    "    tensor = torch.from_numpy(image).unsqueeze(0).unsqueeze(0).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        logits = sliding_window_inference(\n",
    "            tensor,\n",
    "            roi_size=ROI_SIZE,\n",
    "            sw_batch_size=BATCH_SIZE,\n",
    "            predictor=model\n",
    "        )\n",
    "        probabilities = F.softmax(logits, dim=1)\n",
    "\n",
    "    return probabilities.squeeze(0).cpu().numpy()  # Shape: (C, H, W)\n",
    "\n",
    "\n",
    "# PROCESS A SINGLE FILE\n",
    "\n",
    "def process_file(image_path: Path, file_index: int, total_files: int):\n",
    "    print(f\"\\n[{file_index + 1}/{total_files}] Processing: {image_path.name}\")\n",
    "    image_stack = load_image(image_path)\n",
    "    depth, height, width = image_stack.shape\n",
    "\n",
    "    test_probs = predict(image_stack[0], model)\n",
    "    total_classes = test_probs.shape[0]\n",
    "\n",
    "    for cls in EXPORT_CLASSES:\n",
    "        if cls < 0 or cls >= total_classes:\n",
    "            raise ValueError(f\"Invalid class index {cls}. Model returns {total_classes} classes.\")\n",
    "\n",
    "    suffix = \"classification\" if EXPORT_MODE == \"classification\" else \"probability\"\n",
    "    save_path = OUTPUT_DIR / f\"{image_path.stem}_{suffix}.ome.tif\"\n",
    "\n",
    "    print(f\"Predicting {depth} slices with {len(EXPORT_CLASSES)} channels...\")\n",
    "\n",
    "    # Allocate array for final result: shape (C, Z, Y, X)\n",
    "    output_stack = np.zeros((len(EXPORT_CLASSES), depth, height, width), dtype=np.uint8)\n",
    "\n",
    "    for z in range(depth):\n",
    "        prob_map = predict(image_stack[z], model)\n",
    "\n",
    "        if EXPORT_MODE == \"probability\":\n",
    "            out_slice = np.stack([\n",
    "                (prob_map[cls] * 255).astype(np.uint8)\n",
    "                for cls in EXPORT_CLASSES\n",
    "            ], axis=0)\n",
    "\n",
    "        elif EXPORT_MODE == \"classification\":\n",
    "            class_map = np.argmax(prob_map, axis=0)\n",
    "            out_slice = np.stack([\n",
    "                ((class_map == cls).astype(np.uint8)) * 255\n",
    "                for cls in EXPORT_CLASSES\n",
    "            ], axis=0)\n",
    "\n",
    "        output_stack[:, z, :, :] = out_slice  # Fill slice for all classes at Z=z\n",
    "\n",
    "        # Progress bar\n",
    "        bar_length = 30\n",
    "        progress = int((z + 1) / depth * bar_length)\n",
    "        bar = 'â–ˆ' * progress + '-' * (bar_length - progress)\n",
    "        sys.stdout.write(f\"\\r[{bar}] Slice {z + 1}/{depth}\")\n",
    "        sys.stdout.flush()\n",
    "\n",
    "    # Write all slices in one go\n",
    "    print(f\"\\nSaving to: {save_path}\")\n",
    "    tifffile.imwrite(\n",
    "        save_path,\n",
    "        data=output_stack,\n",
    "        photometric='minisblack',\n",
    "        metadata={'axes': 'CZYX'},\n",
    "        bigtiff=True\n",
    "    )\n",
    "    print(f\" Saved OME-TIFF: {save_path}\")\n",
    "    \n",
    "    # garbage \n",
    "    del image_stack\n",
    "    del output_stack\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "       \n",
    "\n",
    "\n",
    "# MAIN SCRIPT\n",
    "\n",
    "def main():\n",
    "    input_files = []\n",
    "    for ext in SUPPORTED_EXTENSIONS:\n",
    "        input_files.extend(DATA_DIR.glob(ext))\n",
    "    input_files = sorted(input_files)\n",
    "\n",
    "    if not input_files:\n",
    "        print(\"No input image files found in the specified folder.\")\n",
    "        return\n",
    "\n",
    "    print(f\"{len(input_files)} image file(s) detected.\")\n",
    "    for idx, image_file in enumerate(input_files):\n",
    "        print(\"\")  # spacing\n",
    "        process_file(image_file, idx, len(input_files))\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "725e240b-c07b-435c-8344-cacea8b32f39",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "958f0ac4-a688-4b4f-beb1-e92ab3f2c325",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vessels_lightsheet",
   "language": "python",
   "name": "mon_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
