{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7c2f80e3-c82a-4ab5-b7c1-14403af52334",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "raster_geometry not imported.  This is only needed for the ellipsoid rendering in apply_stardist\n",
      "2 image file(s) detected.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files:   0%|                                                       | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[1/2] Processing: M21E2-Stitching-01-Create Image Subset-08.czi\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                    "
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Invalid class index 2. Model output has 2 classes.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 135\u001b[39m\n\u001b[32m    132\u001b[39m         process_file(image_file, idx, \u001b[38;5;28mlen\u001b[39m(input_files))\n\u001b[32m    134\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[34m__name__\u001b[39m == \u001b[33m\"\u001b[39m\u001b[33m__main__\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m135\u001b[39m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 132\u001b[39m, in \u001b[36mmain\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    130\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(input_files)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m image file(s) detected.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    131\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m idx, image_file \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(tqdm(input_files, desc=\u001b[33m\"\u001b[39m\u001b[33mProcessing files\u001b[39m\u001b[33m\"\u001b[39m, ncols=\u001b[32m100\u001b[39m, leave=\u001b[38;5;28;01mFalse\u001b[39;00m)):\n\u001b[32m--> \u001b[39m\u001b[32m132\u001b[39m     \u001b[43mprocess_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43minput_files\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 83\u001b[39m, in \u001b[36mprocess_file\u001b[39m\u001b[34m(image_path, file_index, total_files)\u001b[39m\n\u001b[32m     81\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m \u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01min\u001b[39;00m EXPORT_CLASSES:\n\u001b[32m     82\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcls\u001b[39m < \u001b[32m0\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mcls\u001b[39m >= total_classes:\n\u001b[32m---> \u001b[39m\u001b[32m83\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mInvalid class index \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m. Model output has \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtotal_classes\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m classes.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     85\u001b[39m suffix = \u001b[33m\"\u001b[39m\u001b[33mclassification\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m EXPORT_MODE == \u001b[33m\"\u001b[39m\u001b[33mclassification\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mprobability\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     86\u001b[39m save_path = OUTPUT_DIR / \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mimage_path.stem\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00msuffix\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.ome.tif\u001b[39m\u001b[33m\"\u001b[39m\n",
      "\u001b[31mValueError\u001b[39m: Invalid class index 2. Model output has 2 classes."
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import tifffile\n",
    "import czifile\n",
    "from tqdm import tqdm\n",
    "from monai.inferers import sliding_window_inference\n",
    "from tnia.deeplearning.dl_helper import quantile_normalization\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# --------------------------- CONFIGURATION --------------------------- #\n",
    "\n",
    "BASE_PATH = Path(r'C:/Users/Alex/Desktop/Mailis')\n",
    "DATA_DIR = BASE_PATH / \"data\"\n",
    "MODEL_PATH = BASE_PATH / \"models\" / \"full_brain.pth\"\n",
    "OUTPUT_DIR = BASE_PATH / \"Brain_volume\"\n",
    "\n",
    "ROI_SIZE = 1024\n",
    "BATCH_SIZE = 1\n",
    "\n",
    "# NOTE:\n",
    "# In Napari, class labels usually start at 1.\n",
    "# Here, class indices start at 0 (as used by the model).\n",
    "# So class 1 in Napari = class 0 in EXPORT_CLASSES.\n",
    "EXPORT_MODE = \"classification\"  # \"classification\" or \"probability\"\n",
    "EXPORT_CLASSES = [0, 1]       # These are model class indices (starting at 0)\n",
    "\n",
    "SUPPORTED_EXTENSIONS = [\n",
    "    \"*.czi\", \"*.tif\", \"*.tiff\", \"*.ome.tif\", \"*.ome.tiff\", \"*.btf\", \"*.ome.btf\"\n",
    "]\n",
    "\n",
    "# --------------------------- INITIALIZATION --------------------------- #\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = torch.load(MODEL_PATH, weights_only=False).to(device)\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# --------------------------- IMAGE LOADING --------------------------- #\n",
    "\n",
    "def load_image(path: Path) -> np.ndarray:\n",
    "    ext = path.suffix.lower()\n",
    "    if ext == \".czi\":\n",
    "        image = czifile.imread(path)\n",
    "    elif ext in [\".tif\", \".tiff\", \".ome.tif\", \".ome.tiff\", \".btf\", \".ome.btf\"]:\n",
    "        image = tifffile.imread(path)\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported file format: {ext}\")\n",
    "    \n",
    "    image = np.squeeze(image)\n",
    "    if image.ndim != 3:\n",
    "        raise ValueError(f\"Expected 3D image (Z, Y, X), got shape {image.shape}\")\n",
    "    return image\n",
    "\n",
    "# --------------------------- PREDICTION --------------------------- #\n",
    "\n",
    "def predict(image_2d: np.ndarray, model: torch.nn.Module) -> np.ndarray:\n",
    "    image = quantile_normalization(image_2d).astype(np.float32)\n",
    "    tensor = torch.from_numpy(image).unsqueeze(0).unsqueeze(0).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        logits = sliding_window_inference(\n",
    "            tensor,\n",
    "            roi_size=ROI_SIZE,\n",
    "            sw_batch_size=BATCH_SIZE,\n",
    "            predictor=model\n",
    "        )\n",
    "        probabilities = F.softmax(logits, dim=1)\n",
    "\n",
    "    return probabilities.squeeze(0).cpu().numpy()  # Shape: (C, H, W)\n",
    "\n",
    "# --------------------------- PROCESS A SINGLE FILE --------------------------- #\n",
    "\n",
    "def process_file(image_path: Path, file_index: int, total_files: int):\n",
    "    print(f\"\\n[{file_index + 1}/{total_files}] Processing: {image_path.name}\")\n",
    "    image_stack = load_image(image_path)\n",
    "    depth, height, width = image_stack.shape\n",
    "\n",
    "    test_probs = predict(image_stack[0], model)\n",
    "    total_classes = test_probs.shape[0]\n",
    "\n",
    "    for cls in EXPORT_CLASSES:\n",
    "        if cls < 0 or cls >= total_classes:\n",
    "            raise ValueError(f\"Invalid class index {cls}. Model output has {total_classes} classes.\")\n",
    "\n",
    "    suffix = \"classification\" if EXPORT_MODE == \"classification\" else \"probability\"\n",
    "    save_path = OUTPUT_DIR / f\"{image_path.stem}_{suffix}.ome.tif\"\n",
    "\n",
    "    with tifffile.TiffWriter(save_path, bigtiff=True) as tif:\n",
    "        for z in tqdm(range(depth), desc=\"Predicting slices\", leave=False, ncols=100, ascii=True):\n",
    "            prob_map = predict(image_stack[z], model)  # Shape: (C, H, W)\n",
    "\n",
    "            if EXPORT_MODE == \"probability\":\n",
    "                # Stack selected channels scaled to 0â€“255\n",
    "                selected = np.stack([\n",
    "                    (prob_map[cls] * 255).astype(np.uint8)\n",
    "                    for cls in EXPORT_CLASSES\n",
    "                ], axis=0)  # Shape: (C_selected, H, W)\n",
    "\n",
    "            elif EXPORT_MODE == \"classification\":\n",
    "                class_map = np.argmax(prob_map, axis=0)  # Shape: (H, W)\n",
    "                selected = np.stack([\n",
    "                    (class_map == cls).astype(np.uint8) * 255\n",
    "                    for cls in EXPORT_CLASSES\n",
    "                ], axis=0)  # Shape: (C_selected, H, W)\n",
    "\n",
    "            # Add an artificial Z-dim (1) for correct CZYX writing\n",
    "            selected = selected[:, np.newaxis, :, :]  # Shape: (C, 1, Y, X)\n",
    "\n",
    "            tif.write(\n",
    "                selected,\n",
    "                photometric='minisblack',\n",
    "                metadata={'axes': 'CZYX'},\n",
    "                contiguous=True\n",
    "            )\n",
    "\n",
    "    print(f\"Saved output to: {save_path}\")\n",
    "\n",
    "# --------------------------- MAIN SCRIPT --------------------------- #\n",
    "\n",
    "def main():\n",
    "    input_files = []\n",
    "    for ext in SUPPORTED_EXTENSIONS:\n",
    "        input_files.extend(DATA_DIR.glob(ext))\n",
    "    input_files = sorted(input_files)\n",
    "\n",
    "    if not input_files:\n",
    "        print(\"No input image files found in the specified folder.\")\n",
    "        return\n",
    "\n",
    "    print(f\"{len(input_files)} image file(s) detected.\")\n",
    "    for idx, image_file in enumerate(tqdm(input_files, desc=\"Processing files\", ncols=100, leave=False)):\n",
    "        process_file(image_file, idx, len(input_files))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d07fa07-c8d5-4c9d-9767-2d29839d35b7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
